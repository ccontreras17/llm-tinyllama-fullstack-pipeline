{
  "best_global_step": 1350,
  "best_metric": 1.0542713403701782,
  "best_model_checkpoint": "/home/charlie/Chaos-Projects/Python/Wednesday/lora_adapters/python_light/checkpoint-1350",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1350,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 0.5417275428771973,
      "learning_rate": 4.966666666666667e-05,
      "loss": 2.4192,
      "step": 10
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 0.6618666648864746,
      "learning_rate": 4.92962962962963e-05,
      "loss": 2.3651,
      "step": 20
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.813705563545227,
      "learning_rate": 4.892592592592593e-05,
      "loss": 2.4503,
      "step": 30
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 1.0456209182739258,
      "learning_rate": 4.855555555555556e-05,
      "loss": 2.2685,
      "step": 40
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 1.0484685897827148,
      "learning_rate": 4.818518518518519e-05,
      "loss": 2.0912,
      "step": 50
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.547365665435791,
      "learning_rate": 4.7814814814814816e-05,
      "loss": 2.0937,
      "step": 60
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 1.0529667139053345,
      "learning_rate": 4.7444444444444445e-05,
      "loss": 1.9563,
      "step": 70
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.2875622510910034,
      "learning_rate": 4.7074074074074074e-05,
      "loss": 1.8689,
      "step": 80
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.6320229768753052,
      "learning_rate": 4.67037037037037e-05,
      "loss": 2.0191,
      "step": 90
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 1.6130101680755615,
      "learning_rate": 4.633333333333333e-05,
      "loss": 1.7421,
      "step": 100
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 1.7564536333084106,
      "learning_rate": 4.596296296296296e-05,
      "loss": 1.5867,
      "step": 110
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.2518980503082275,
      "learning_rate": 4.559259259259259e-05,
      "loss": 1.4972,
      "step": 120
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 1.5286623239517212,
      "learning_rate": 4.522222222222223e-05,
      "loss": 1.4698,
      "step": 130
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 1.5558819770812988,
      "learning_rate": 4.4851851851851856e-05,
      "loss": 1.2771,
      "step": 140
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.3980082273483276,
      "learning_rate": 4.4481481481481485e-05,
      "loss": 1.4399,
      "step": 150
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 1.3888540267944336,
      "learning_rate": 4.4111111111111114e-05,
      "loss": 1.2822,
      "step": 160
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 1.3449496030807495,
      "learning_rate": 4.374074074074074e-05,
      "loss": 1.1812,
      "step": 170
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.487741470336914,
      "learning_rate": 4.337037037037037e-05,
      "loss": 1.1844,
      "step": 180
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 1.6299549341201782,
      "learning_rate": 4.3e-05,
      "loss": 1.2266,
      "step": 190
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.5964806079864502,
      "learning_rate": 4.262962962962963e-05,
      "loss": 1.0894,
      "step": 200
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.611249566078186,
      "learning_rate": 4.225925925925926e-05,
      "loss": 1.131,
      "step": 210
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 1.4594950675964355,
      "learning_rate": 4.188888888888889e-05,
      "loss": 0.987,
      "step": 220
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 1.8225452899932861,
      "learning_rate": 4.1518518518518525e-05,
      "loss": 1.1533,
      "step": 230
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.9791654348373413,
      "learning_rate": 4.1148148148148154e-05,
      "loss": 0.9792,
      "step": 240
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.4111343622207642,
      "learning_rate": 4.0777777777777783e-05,
      "loss": 1.0808,
      "step": 250
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 1.2781853675842285,
      "learning_rate": 4.040740740740741e-05,
      "loss": 1.1387,
      "step": 260
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.515594720840454,
      "learning_rate": 4.003703703703704e-05,
      "loss": 1.077,
      "step": 270
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 1.5742344856262207,
      "learning_rate": 3.966666666666667e-05,
      "loss": 1.0221,
      "step": 280
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 1.8234758377075195,
      "learning_rate": 3.92962962962963e-05,
      "loss": 1.097,
      "step": 290
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.4883198738098145,
      "learning_rate": 3.892592592592593e-05,
      "loss": 1.1704,
      "step": 300
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 1.6647735834121704,
      "learning_rate": 3.855555555555556e-05,
      "loss": 0.9829,
      "step": 310
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.6660622358322144,
      "learning_rate": 3.818518518518519e-05,
      "loss": 1.0927,
      "step": 320
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 1.3889819383621216,
      "learning_rate": 3.781481481481482e-05,
      "loss": 1.0793,
      "step": 330
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 1.8767794370651245,
      "learning_rate": 3.7444444444444446e-05,
      "loss": 1.0863,
      "step": 340
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 1.5130157470703125,
      "learning_rate": 3.7074074074074075e-05,
      "loss": 1.1047,
      "step": 350
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.2841339111328125,
      "learning_rate": 3.6703703703703704e-05,
      "loss": 1.0648,
      "step": 360
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 1.590465784072876,
      "learning_rate": 3.633333333333333e-05,
      "loss": 1.1501,
      "step": 370
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 1.5815153121948242,
      "learning_rate": 3.596296296296296e-05,
      "loss": 1.0206,
      "step": 380
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 1.3487592935562134,
      "learning_rate": 3.559259259259259e-05,
      "loss": 1.0807,
      "step": 390
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.3804346323013306,
      "learning_rate": 3.522222222222222e-05,
      "loss": 0.9031,
      "step": 400
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 1.7672059535980225,
      "learning_rate": 3.485185185185185e-05,
      "loss": 1.0683,
      "step": 410
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.945723295211792,
      "learning_rate": 3.448148148148148e-05,
      "loss": 1.1419,
      "step": 420
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 1.501096487045288,
      "learning_rate": 3.411111111111111e-05,
      "loss": 1.0802,
      "step": 430
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 2.5358471870422363,
      "learning_rate": 3.3740740740740744e-05,
      "loss": 0.9894,
      "step": 440
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.9885505437850952,
      "learning_rate": 3.337037037037037e-05,
      "loss": 0.9933,
      "step": 450
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.1046724319458008,
      "eval_runtime": 6.6919,
      "eval_samples_per_second": 29.887,
      "eval_steps_per_second": 3.736,
      "step": 450
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 1.6872775554656982,
      "learning_rate": 3.3e-05,
      "loss": 1.08,
      "step": 460
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 1.531509518623352,
      "learning_rate": 3.262962962962963e-05,
      "loss": 0.988,
      "step": 470
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 2.793344020843506,
      "learning_rate": 3.225925925925926e-05,
      "loss": 0.9852,
      "step": 480
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 1.4859871864318848,
      "learning_rate": 3.188888888888889e-05,
      "loss": 0.9815,
      "step": 490
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 2.389615297317505,
      "learning_rate": 3.151851851851852e-05,
      "loss": 1.034,
      "step": 500
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 1.1056711673736572,
      "learning_rate": 3.114814814814815e-05,
      "loss": 1.0327,
      "step": 510
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 1.4749608039855957,
      "learning_rate": 3.077777777777778e-05,
      "loss": 1.0553,
      "step": 520
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 1.7930594682693481,
      "learning_rate": 3.0407407407407407e-05,
      "loss": 1.0401,
      "step": 530
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.779455304145813,
      "learning_rate": 3.0037037037037036e-05,
      "loss": 1.0677,
      "step": 540
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 2.842156171798706,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 1.1475,
      "step": 550
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 1.8463654518127441,
      "learning_rate": 2.92962962962963e-05,
      "loss": 0.9874,
      "step": 560
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 1.6396516561508179,
      "learning_rate": 2.892592592592593e-05,
      "loss": 1.1265,
      "step": 570
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 1.673025369644165,
      "learning_rate": 2.855555555555556e-05,
      "loss": 0.9052,
      "step": 580
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 2.090791940689087,
      "learning_rate": 2.8185185185185185e-05,
      "loss": 1.1327,
      "step": 590
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.0395781993865967,
      "learning_rate": 2.7814814814814814e-05,
      "loss": 0.987,
      "step": 600
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 2.004077911376953,
      "learning_rate": 2.7444444444444443e-05,
      "loss": 0.8955,
      "step": 610
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 1.8039277791976929,
      "learning_rate": 2.7074074074074072e-05,
      "loss": 0.9006,
      "step": 620
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.6744933128356934,
      "learning_rate": 2.67037037037037e-05,
      "loss": 1.0161,
      "step": 630
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 1.8718547821044922,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.9758,
      "step": 640
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 1.8307291269302368,
      "learning_rate": 2.5962962962962967e-05,
      "loss": 1.0858,
      "step": 650
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 1.69069504737854,
      "learning_rate": 2.5592592592592596e-05,
      "loss": 1.0473,
      "step": 660
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 1.268080711364746,
      "learning_rate": 2.5222222222222225e-05,
      "loss": 0.997,
      "step": 670
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 1.7944550514221191,
      "learning_rate": 2.4851851851851854e-05,
      "loss": 1.0079,
      "step": 680
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 2.2949392795562744,
      "learning_rate": 2.4481481481481483e-05,
      "loss": 0.9687,
      "step": 690
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 1.9007184505462646,
      "learning_rate": 2.4111111111111113e-05,
      "loss": 1.0311,
      "step": 700
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 1.8154362440109253,
      "learning_rate": 2.3740740740740742e-05,
      "loss": 0.9885,
      "step": 710
    },
    {
      "epoch": 1.6,
      "grad_norm": 2.0529627799987793,
      "learning_rate": 2.337037037037037e-05,
      "loss": 0.9352,
      "step": 720
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 1.208847165107727,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.9321,
      "step": 730
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 1.8506639003753662,
      "learning_rate": 2.2629629629629633e-05,
      "loss": 0.9506,
      "step": 740
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.914349913597107,
      "learning_rate": 2.2259259259259262e-05,
      "loss": 1.1371,
      "step": 750
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 2.3072423934936523,
      "learning_rate": 2.188888888888889e-05,
      "loss": 1.0954,
      "step": 760
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 1.9013372659683228,
      "learning_rate": 2.151851851851852e-05,
      "loss": 0.9173,
      "step": 770
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 1.4402642250061035,
      "learning_rate": 2.114814814814815e-05,
      "loss": 0.9644,
      "step": 780
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 1.5014121532440186,
      "learning_rate": 2.077777777777778e-05,
      "loss": 0.9253,
      "step": 790
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 1.6988956928253174,
      "learning_rate": 2.0407407407407408e-05,
      "loss": 0.8916,
      "step": 800
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.0730433464050293,
      "learning_rate": 2.0037037037037037e-05,
      "loss": 0.9864,
      "step": 810
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 1.71649968624115,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.8524,
      "step": 820
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 2.057729721069336,
      "learning_rate": 1.92962962962963e-05,
      "loss": 1.0749,
      "step": 830
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.800405740737915,
      "learning_rate": 1.8925925925925928e-05,
      "loss": 1.0243,
      "step": 840
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.5112565755844116,
      "learning_rate": 1.8555555555555557e-05,
      "loss": 1.0595,
      "step": 850
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 2.2502286434173584,
      "learning_rate": 1.8185185185185186e-05,
      "loss": 0.8859,
      "step": 860
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 1.9276349544525146,
      "learning_rate": 1.7814814814814815e-05,
      "loss": 0.9585,
      "step": 870
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 1.8384075164794922,
      "learning_rate": 1.7444444444444448e-05,
      "loss": 0.9548,
      "step": 880
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 2.221416711807251,
      "learning_rate": 1.7074074074074077e-05,
      "loss": 0.9509,
      "step": 890
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.533165693283081,
      "learning_rate": 1.6703703703703706e-05,
      "loss": 0.8957,
      "step": 900
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.0626248121261597,
      "eval_runtime": 6.6882,
      "eval_samples_per_second": 29.903,
      "eval_steps_per_second": 3.738,
      "step": 900
    },
    {
      "epoch": 2.022222222222222,
      "grad_norm": 1.8382611274719238,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.8822,
      "step": 910
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 2.1057612895965576,
      "learning_rate": 1.5962962962962964e-05,
      "loss": 0.9248,
      "step": 920
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 2.162236452102661,
      "learning_rate": 1.5592592592592593e-05,
      "loss": 0.9688,
      "step": 930
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 2.0444962978363037,
      "learning_rate": 1.5222222222222224e-05,
      "loss": 0.932,
      "step": 940
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 1.688245415687561,
      "learning_rate": 1.4851851851851853e-05,
      "loss": 0.9348,
      "step": 950
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 1.95936918258667,
      "learning_rate": 1.4481481481481483e-05,
      "loss": 0.9235,
      "step": 960
    },
    {
      "epoch": 2.1555555555555554,
      "grad_norm": 1.8016432523727417,
      "learning_rate": 1.4111111111111112e-05,
      "loss": 1.0096,
      "step": 970
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 1.6989530324935913,
      "learning_rate": 1.3740740740740741e-05,
      "loss": 0.9332,
      "step": 980
    },
    {
      "epoch": 2.2,
      "grad_norm": 2.0072646141052246,
      "learning_rate": 1.3370370370370372e-05,
      "loss": 1.0927,
      "step": 990
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 2.672693967819214,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.0157,
      "step": 1000
    },
    {
      "epoch": 2.2444444444444445,
      "grad_norm": 2.1854255199432373,
      "learning_rate": 1.262962962962963e-05,
      "loss": 1.0638,
      "step": 1010
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 1.5490037202835083,
      "learning_rate": 1.225925925925926e-05,
      "loss": 0.9394,
      "step": 1020
    },
    {
      "epoch": 2.2888888888888888,
      "grad_norm": 2.1166632175445557,
      "learning_rate": 1.188888888888889e-05,
      "loss": 1.016,
      "step": 1030
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 1.8083008527755737,
      "learning_rate": 1.151851851851852e-05,
      "loss": 0.9956,
      "step": 1040
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 2.8479983806610107,
      "learning_rate": 1.1148148148148148e-05,
      "loss": 1.0793,
      "step": 1050
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 1.828952431678772,
      "learning_rate": 1.0777777777777778e-05,
      "loss": 0.9369,
      "step": 1060
    },
    {
      "epoch": 2.3777777777777778,
      "grad_norm": 1.63653564453125,
      "learning_rate": 1.0407407407407407e-05,
      "loss": 0.9559,
      "step": 1070
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.5890254974365234,
      "learning_rate": 1.0037037037037038e-05,
      "loss": 0.8595,
      "step": 1080
    },
    {
      "epoch": 2.422222222222222,
      "grad_norm": 2.6817586421966553,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.9657,
      "step": 1090
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 1.982706069946289,
      "learning_rate": 9.296296296296298e-06,
      "loss": 1.104,
      "step": 1100
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 1.993920922279358,
      "learning_rate": 8.925925925925927e-06,
      "loss": 0.9735,
      "step": 1110
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 1.9744691848754883,
      "learning_rate": 8.555555555555556e-06,
      "loss": 0.8497,
      "step": 1120
    },
    {
      "epoch": 2.511111111111111,
      "grad_norm": 1.6289671659469604,
      "learning_rate": 8.185185185185187e-06,
      "loss": 0.8568,
      "step": 1130
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 1.9497860670089722,
      "learning_rate": 7.814814814814816e-06,
      "loss": 0.9615,
      "step": 1140
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 1.96466863155365,
      "learning_rate": 7.444444444444444e-06,
      "loss": 1.0603,
      "step": 1150
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 1.589793086051941,
      "learning_rate": 7.074074074074074e-06,
      "loss": 1.0202,
      "step": 1160
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.8084815740585327,
      "learning_rate": 6.703703703703703e-06,
      "loss": 1.0205,
      "step": 1170
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 2.1081371307373047,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.9362,
      "step": 1180
    },
    {
      "epoch": 2.6444444444444444,
      "grad_norm": 1.9273767471313477,
      "learning_rate": 5.962962962962963e-06,
      "loss": 1.0454,
      "step": 1190
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 2.199937582015991,
      "learning_rate": 5.592592592592593e-06,
      "loss": 1.0123,
      "step": 1200
    },
    {
      "epoch": 2.688888888888889,
      "grad_norm": 1.830595850944519,
      "learning_rate": 5.2222222222222226e-06,
      "loss": 0.9668,
      "step": 1210
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 1.6878911256790161,
      "learning_rate": 4.851851851851852e-06,
      "loss": 1.0163,
      "step": 1220
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 2.3465683460235596,
      "learning_rate": 4.481481481481482e-06,
      "loss": 0.9379,
      "step": 1230
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 1.9052186012268066,
      "learning_rate": 4.111111111111112e-06,
      "loss": 0.9369,
      "step": 1240
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 2.723689317703247,
      "learning_rate": 3.7407407407407413e-06,
      "loss": 1.0397,
      "step": 1250
    },
    {
      "epoch": 2.8,
      "grad_norm": 2.0225393772125244,
      "learning_rate": 3.37037037037037e-06,
      "loss": 0.914,
      "step": 1260
    },
    {
      "epoch": 2.822222222222222,
      "grad_norm": 1.6398913860321045,
      "learning_rate": 3e-06,
      "loss": 0.9446,
      "step": 1270
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 2.0700957775115967,
      "learning_rate": 2.6296296296296297e-06,
      "loss": 0.9363,
      "step": 1280
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 1.861124038696289,
      "learning_rate": 2.2592592592592592e-06,
      "loss": 0.9554,
      "step": 1290
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 1.5232857465744019,
      "learning_rate": 1.888888888888889e-06,
      "loss": 0.9553,
      "step": 1300
    },
    {
      "epoch": 2.911111111111111,
      "grad_norm": 1.9633082151412964,
      "learning_rate": 1.5185185185185186e-06,
      "loss": 0.9107,
      "step": 1310
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 1.7132008075714111,
      "learning_rate": 1.1481481481481482e-06,
      "loss": 0.9682,
      "step": 1320
    },
    {
      "epoch": 2.9555555555555557,
      "grad_norm": 1.7531108856201172,
      "learning_rate": 7.777777777777778e-07,
      "loss": 0.9864,
      "step": 1330
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 1.6797597408294678,
      "learning_rate": 4.0740740740740737e-07,
      "loss": 0.9622,
      "step": 1340
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.824139952659607,
      "learning_rate": 3.7037037037037036e-08,
      "loss": 1.0179,
      "step": 1350
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.0542713403701782,
      "eval_runtime": 6.7183,
      "eval_samples_per_second": 29.769,
      "eval_steps_per_second": 3.721,
      "step": 1350
    }
  ],
  "logging_steps": 10,
  "max_steps": 1350,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4291603778764800.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
